---
tags:
  - articles/research
  - notes
url: https://arxiv.org/abs/2010.11929
code: true
---

> [!help] Resources 
> üìÑ **Article**
> [[Vision-Transformers.pdf|Dosovitskiy, An Image is Worth 16x16 Words]]
> 
> :luc_github: GitHub
> [Implementation](https://github.com/google-research/vision_transformer)



> [!quote] Abstract
> While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring sub- stantially fewer computational resources to train.

# Introduction

## Rise of Self-Attention Based Models

[[Attention Mechanism and Transformers|Self-Attention based architectures]], **Transformers** in particular, have become the model of choice for natural language processing tasks.

### Approach

1. Pre-train first on *large corpus* of text.
2. Fine-tune on smaller, *task-specific* text.

### Effectiveness

- Due to computational *efficiency and scalability*, possible to train models upto 100B params! üò±
- Even with growing size of datasets and models, *no sign of saturation in performance*. üí™

## Combination with Convolutional Architectures

In computer vision, *convolution-based architectures* are still dominating. Inspired by the effectiveness of transformers, multiple works have attempted to combine CNNs with self-attention.

### Current Drawbacks

- Models theoretically efficient, but neither tested nor scaled effectively on modern hardware accelertors due to use of specialized attention-patterns.
- So ResNet architectures still "state-of-the-art" in image classification landscape.

## Patches as Tokens

> :luc_quote: Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard Transformer directly to images, with the fewest possible modifications. To do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer. Image patches are treated the same way as tokens (words) in an NLP application. We train the model on image classification in supervised fashion.

1. Applying standard transformer directly to images, with fewest possible mods.
2. Split image into patches.
3. Linear embedding on each patch.
4. Sequence of embeddings -> Transformer, *just like usual tokens*
5. Train model on image classification data in supervised fashion.

## Size Matters

### Mid-Sized Datasets üê§

- When trained on **mid-sized** datasets like [ImageNet](https://www.image-net.org/) *without strong regularization*, accuracy is ==a few percentage points below ResNets of comparable size==.
- This is expected, as Transformers lack some of the inductive biases inherent to CNNs, like *translational equivariance* and *locality*. So they ==cannot generalize well on insufficient amounts of data==.

> [!important] Properties of CNNs
> #### Translational Equivariance
> 
> 1. Translation equivariance is a fundamental property of Convolutional Neural Networks (CNNs) that allows them to *maintain the spatial relationships of features* when the input image is translated.
> 2. Translation equivariance refers to the ability of a function or system to produce ==outputs that shift in the same way as its inputs when those inputs are translated==. In the context of CNNs, if an input image is translated (shifted), the output feature maps generated by the convolutional layers will also be translated by the same amount.
> [:luc_link_2: blog.papersapce](https://blog.paperspace.com/pooling-and-translation-invariance-in-convolutional-neural-networks/)
> [:luc_link_2: datascience.stackexchange](https://datascience.stackexchange.com/questions/16060/what-is-the-difference-between-equivariant-to-translation-and-invariant-to-tr)
> 
> #### Locality
> 
> Locality in Convolutional Neural Networks (CNNs) refers to the principle that ==nearby pixels in an image are more likely to be correlated than distant pixels==. This concept is crucial for effectively processing visual data, as it allows CNNs to *focus on local patterns and features within an image*, rather than treating all pixels as independent.

### Large Datasets üêî

When trained on enormous datasets of 14M-300M examples, large scale training trumps inductive bias. Pre-trained on [ImageNet 21k](https://arxiv.org/abs/2104.10972), [JFT-300M](https://paperswithcode.com/dataset/jft-300m), ==ViT beats state of the art models on image recognition benchmarks==.

| Dataset             | **Accuracy** |
| ------------------- | ------------ |
| **ImageNet**        | 88.55%       |
| **ImageNet-ReaL**   | 90.72%       |
| **CIFAR-100**       | 94.55%       |
| **VTAB (19 tasks)** | 77.63%       |

# Methodology

![[bs-thesis/notes/assets/vision-transformer__methodology.png]]
